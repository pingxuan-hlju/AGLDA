{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c71d49f",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd26d99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T00:56:43.330973Z",
     "start_time": "2022-10-07T00:56:41.975223Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4481500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T02:37:53.118260Z",
     "start_time": "2022-10-01T02:37:53.114321Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0860e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T00:56:44.945784Z",
     "start_time": "2022-10-07T00:56:44.892849Z"
    }
   },
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e90f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T02:37:53.111148Z",
     "start_time": "2022-10-01T02:37:53.082919Z"
    }
   },
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbfef827",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b32de84",
   "metadata": {},
   "source": [
    "## data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979511d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T02:37:53.125173Z",
     "start_time": "2022-10-01T02:37:53.120504Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():  # lnc 240 dis 405 mi 495\n",
    "    ld=np.loadtxt(\"./data/lnc_dis_association.txt\",dtype=int)\n",
    "    dd=np.loadtxt(\"./data/dis_sim_matrix_process.txt\",dtype=float)\n",
    "    md=np.loadtxt(\"./data/mi_dis.txt\",dtype=int)\n",
    "    lm=np.loadtxt(\"./data/yuguoxian_lnc_mi.txt\",dtype=int)\n",
    "    return torch.tensor(ld),torch.tensor(dd),torch.tensor(md),torch.tensor(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f4e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T02:37:53.132310Z",
     "start_time": "2022-10-01T02:37:53.126851Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_sim(ld,dd):\n",
    "    s1=ld.shape[0]\n",
    "    ll=torch.eye(s1)\n",
    "    m2=dd*ld[:,None,:]\n",
    "    m1=ld[:,:,None]\n",
    "    for x,y in itertools.permutations(torch.linspace(0,s1-1,s1,dtype=torch.long),2):\n",
    "        x,y=x.item(),y.item()\n",
    "        m=m1[x,:,:]*m2[y,:,:]\n",
    "        if ld[x].sum()+ld[y].sum()==0:\n",
    "            ll[x,y]=0\n",
    "        else:\n",
    "            ll[x,y]=(m.max(dim=0,keepdim=True)[0].sum()+m.max(dim=1,keepdim=True)[0].sum())/(ld[x].sum()+ld[y].sum())\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83ff86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T02:37:53.142418Z",
     "start_time": "2022-10-01T02:37:53.133870Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(al,dd,n):#5 cross\n",
    "    rand_index=torch.randperm(al.sum())\n",
    "    ps=torch.argwhere(al==1).index_select(0,rand_index)\n",
    "    ns=torch.argwhere(al==0)\n",
    "    ns=ns.index_select(0,torch.randperm(ns.shape[0]))\n",
    "    sf=int(ps.shape[0]/n)\n",
    "    tri,tei,lda,ll=[],[],[],[]\n",
    "    for i in range(n):\n",
    "        ptrn=torch.cat([ps[:(i*sf),:],ps[((i+1)*sf):(n*sf),:]],dim=0).T\n",
    "        ntrn=torch.cat([ns[:(i*sf),:],ns[((i+1)*sf):(n*sf),:]],dim=0).T\n",
    "        trn=torch.cat([ptrn,ntrn],dim=1)\n",
    "        pten=torch.cat([ps[(i*sf):((i+1)*sf),:],ps[(n*sf):,:]],dim=0).T\n",
    "        nten=torch.cat([ns[(i*sf):((i+1)*sf),:],ns[(n*sf):,:]],dim=0).T\n",
    "        ten=torch.cat([pten,nten],dim=1)\n",
    "        tri.append(trn)\n",
    "        tei.append(ten)\n",
    "        ldt=al.clone()\n",
    "        ldt[pten[0,:],pten[1,:]]=0\n",
    "        lda.append(ldt)\n",
    "        ll.append(calculate_sim(ldt,dd))  \n",
    "    return tri,tei,lda,ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24150c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-01T02:37:52.011Z"
    }
   },
   "outputs": [],
   "source": [
    "n=5\n",
    "ld,dd,md,lm=load_data()\n",
    "tri,tei,lda,ll=split_dataset(ld,dd,n)\n",
    "mm=calculate_sim(md,dd)\n",
    "datasave=(n,ld,dd,md,lm,tri,tei,lda,ll,mm)\n",
    "torch.save(datasave,'tempdata2.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fac5edcf",
   "metadata": {},
   "source": [
    "## data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,ld,dd,md,lm,tri,tei,lda,ll,mm=torch.load('./tempdata2.pth')\n",
    "def chm(m1,m2,th1=0,th2=0,th3=0):\n",
    "    #th is threshold\n",
    "    poi=[]\n",
    "    m1idx=torch.argwhere(m1)\n",
    "    for i in range(m1idx.shape[0]):\n",
    "        p1,p2=m1idx[i]\n",
    "        m2idx=torch.argwhere(m2[p2])\n",
    "        for j in range(m2idx.shape[0]):\n",
    "            poi.append([p1+th1,p2+th2,m2idx[j][0]+th3])\n",
    "    return poi\n",
    "def cfm(ll,ld,dd,md,lm,mm):\n",
    "    #lld ldd lmd\n",
    "    #  e1 e2 e3 ..\n",
    "    #l\n",
    "    #d\n",
    "    #m\n",
    "    #llidx=torch.argwhere(ll)  \n",
    "    #ldidx=torch.argwhere(ld)\n",
    "    #ddidx=torch.argwhere(dd)\n",
    "    #mdidx=torch.argwhere(md)\n",
    "    #lmidx=torch.argwhere(lm)\n",
    "    \n",
    "    r1=torch.cat([ll,ld,lm],dim=1)\n",
    "    r2=torch.cat([ld.T,dd,md.T],dim=1)\n",
    "    r3=torch.cat([lm.T,md,mm],dim=1)\n",
    "    fea=torch.cat([r1,r2,r3],dim=0)\n",
    "    \n",
    "    deg=torch.diag((torch.sum(fea,dim=1))**(-1/2)).double()\n",
    "    #G1=deg@(fea+torch.diag(torch.ones(fea.shape[0])))@deg\n",
    "    G1=deg@fea@deg\n",
    "\n",
    "    '''计算超图G2包含元路径信息\n",
    "    #这里容易使得超图过大，将大矩阵分块计算\n",
    "    pois1=chm(ll,ld,0,0,240)+chm(ld,dd,0,240,240)+chm(lm,md,0,645,240)\n",
    "    pois=pois1\n",
    "    # pois=[]\n",
    "    #for l1 in pois1:   删除冗余。不过代价太大\n",
    "    #    if l1 not in pois:\n",
    "    #        pois.append(l1)\n",
    "    H=[]\n",
    "    Dvs=[]\n",
    "    Des=[]\n",
    "    Hsize=1000\n",
    "    Hl=len(pois)//Hsize\n",
    "    for i in range(Hl+1):\n",
    "        if i==Hl:\n",
    "            Ht=torch.zeros(1140,len(pois)-Hl*Hsize)\n",
    "        else:\n",
    "            Ht=torch.zeros(1140,Hsize)\n",
    "        for j in range(Ht.shape[1]):\n",
    "            Ht[pois[j+i*Hsize],j]=1\n",
    "        Dvs.append(torch.sum(Ht,dim=1,keepdim=True))\n",
    "        Des.append(torch.sum(Ht,dim=0,keepdim=True))\n",
    "        H.append(Ht)\n",
    "    Dv=torch.zeros(1140,1)\n",
    "    for i in range(len(Dvs)):\n",
    "        Dv+=Dvs[i]\n",
    "    Dv=Dv**(-1/2)\n",
    "    Dv[torch.where(Dv==torch.inf)]=0\n",
    "    for i in range(len(Des)):\n",
    "        Des[i]=Des[i]**(-1)\n",
    "    for i in range(len(H)):\n",
    "        H[i]=Dv*H[i]*Des[i]@H[i].T*Dv\n",
    "    G2=torch.zeros(1140,1140)\n",
    "    for i in range(len(H)):\n",
    "        G2+=H[i]\n",
    "    '''\n",
    "    return fea,G1#,G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528669f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feas=[]\n",
    "G1s=[]\n",
    "#G2s=[]\n",
    "for i in range(n):\n",
    "    fea,G1=cfm(ll[i],lda[i],dd,md,lm,mm)\n",
    "    feas.append(fea)\n",
    "    G1s.append(G1)\n",
    "    #G2s.append(G2)\n",
    "torch.save((feas,G1s),'./tempdata3.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc9896f7",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2604dfd2",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968c0fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T00:56:49.116282Z",
     "start_time": "2022-10-07T00:56:49.093252Z"
    }
   },
   "outputs": [],
   "source": [
    "learn_rate=0.0003\n",
    "epoch=80\n",
    "batch=32\n",
    "n,ld,dd,md,lm,tri,tei,lda,ll,mm=torch.load('./parasave/tempdata2.pth')\n",
    "feas,G1s=torch.load('./parasave/tempdata3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afbde2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T00:56:49.404889Z",
     "start_time": "2022-10-07T00:56:49.396923Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):#shuffle and batch from Dataloader\n",
    "    def __init__(self,tri,ld):\n",
    "        self.tri=tri\n",
    "        self.ld=ld\n",
    "    def __getitem__(self,idx):\n",
    "        x,y=self.tri[:,idx]\n",
    "        label=self.ld[x][y]\n",
    "        return x,y,label\n",
    "    def __len__(self):\n",
    "        return self.tri.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cc6d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T00:56:50.063715Z",
     "start_time": "2022-10-07T00:56:50.046694Z"
    }
   },
   "outputs": [],
   "source": [
    "class HGNN_conv(nn.Module):\n",
    "    '''\n",
    "    @article{feng2018hypergraph,\n",
    "    title={Hypergraph Neural Networks},\n",
    "    author={Feng, Yifan and You, Haoxuan and Zhang, Zizhao and Ji, Rongrong and Gao, Yue},\n",
    "    journal={AAAI 2019},\n",
    "    year={2018}\n",
    "    }\n",
    "    '''\n",
    "    def __init__(self, in_ft, out_ft, bias=True):\n",
    "        super(HGNN_conv, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_ft, out_ft))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_ft))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))  \n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, G: torch.Tensor):\n",
    "        x = x.matmul(self.weight)     \n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        x = G.matmul(x)          \n",
    "        return x\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.ne=nn.Parameter(torch.rand(500,300))\n",
    "        self.w1=nn.Parameter(torch.rand(1140,300))\n",
    "        #self.w2=nn.Parameter(torch.randn(1440,300))\n",
    "        self.w2=nn.Linear(1440,300)\n",
    "        self.h3=HGNN_conv(1140,600)\n",
    "        self.h4=HGNN_conv(600,300)\n",
    "        self.h1=HGNN_conv(1140,600)\n",
    "        self.h2=HGNN_conv(600,300)        \n",
    "        self.hf1=nn.Linear(1440*2,50)\n",
    "\n",
    "        self.c11=nn.Conv2d(1,32,kernel_size=(2,7),stride=1,padding=0)\n",
    "        self.s2=nn.MaxPool2d(kernel_size=(1,7))\n",
    "        self.c31=nn.Conv2d(32,64,kernel_size=(1,7),stride=1,padding=0)\n",
    "        self.c32=nn.Conv2d(32,64,kernel_size=(1,7),stride=1,padding=0)\n",
    "        self.s4=nn.MaxPool2d(kernel_size=(1,7))\n",
    "        self.cf1=nn.Linear(22*64,200)#1140->70\n",
    "\n",
    "        self.f3=nn.Linear(250,2)\n",
    "        self.cd1=nn.Dropout(0.5)\n",
    "        self.cd2=nn.Dropout2d(0.5)\n",
    "        \n",
    "        self.leakrelu=nn.LeakyReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "    def forward(self,x1,x2,fea,G1):\n",
    "        \n",
    "        x3=x2+240\n",
    "        x=torch.cat([fea[x1][:,None,None,:],fea[x3][:,None,None,:]],dim=2)\n",
    "        \n",
    "        hyperH=self.leakrelu(G1@self.w1@self.ne.T)\n",
    "        self.H=hyperH\n",
    "        hyperH=(hyperH-hyperH.min())/(hyperH.max()-hyperH.min())\n",
    "        \n",
    "        dv=(torch.sum(hyperH,dim=1)**(-1/2))[:,None]\n",
    "        de=(torch.sum(hyperH,dim=0)**(-1))[None,:]\n",
    "        dv[torch.where(dv==torch.inf)]=0\n",
    "        de[torch.where(de==torch.inf)]=0\n",
    "        hyperH=dv*hyperH*de@hyperH.T*dv\n",
    "\n",
    "        fea1=self.leakrelu(self.h1(fea,G1))\n",
    "        fea2=self.leakrelu(self.h3(fea,hyperH))\n",
    "        fea3=self.leakrelu(self.h2(fea1,G1))\n",
    "        fea4=self.leakrelu(self.h4(fea2+fea1,hyperH))\n",
    "        fea3=torch.cat([fea,fea3+fea4],dim=1)\n",
    "\n",
    "        self.oe=self.w2(self.H.T@fea3)\n",
    "        \n",
    "        fea3=torch.cat([fea3[x1],fea3[x3]],dim=1)\n",
    "        fea3=self.leakrelu(self.hf1(fea3))\n",
    "        \n",
    "        x=self.s2(self.leakrelu(self.c11(x)))\n",
    "        att=self.tanh(self.c32(x))\n",
    "        x=self.s4(self.leakrelu(self.c31(x)*att+self.c31(x)))\n",
    "        x=self.cd2(x)\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        x=self.leakrelu(self.cf1(x))\n",
    "        \n",
    "        x=torch.cat([x,fea3],dim=1)\n",
    "        \n",
    "        x=self.cd1(x)\n",
    "        x=self.f3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def loss(self):\n",
    "        p=torch.softmax(self.oe/1e+7,dim=-1)\n",
    "        q=torch.log_softmax(self.ne,dim=-1)\n",
    "        return nn.KLDivLoss(reduction='batchmean')(q,p)\n",
    "#net=Net()\n",
    "#fea=torch.randn(1140,1140)\n",
    "#G=torch.randn(1140,1140)\n",
    "#x1=torch.linspace(0,31,32).long()\n",
    "#x2=x1\n",
    "#net(x1,x2,fea,G).shape\n",
    "#net.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8cccf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T00:56:50.598277Z",
     "start_time": "2022-10-07T00:56:50.581346Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model,train_set,test_set,fea,G1,tei,epoch,learn_rate,cros):\n",
    "    optimizer=torch.optim.Adam(model.parameters(),learn_rate,weight_decay=0.001)\n",
    "    cost=nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    fea,G1=fea.float().to(device),G1.float().to(device)#,G2.float().to(device)\n",
    "    Amax=[0,0]\n",
    "    for i in range(epoch):\n",
    "        for x1,x2,y in train_set:\n",
    "            x1,x2,y=Variable(x1.long()).to(device),Variable(x2.long()).to(device),Variable(y.long()).to(device)\n",
    "            out=model(x1,x2,fea,G1)\n",
    "            loss=cost(out,y)#+model.loss()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (i+1)%10 == 0 and i+1>=50:\n",
    "            tacc(model,test_set,fea,G1,tei,cros,Amax)\n",
    "        #if i+1==epoch:\n",
    "            #tacc(model,test_set,fea,G1,0,cros)\n",
    "        torch.cuda.empty_cache()\n",
    "'''       \n",
    "def tacc(model,tset,fea,G1,string,cros):\n",
    "    correct=0\n",
    "    total=0\n",
    "    st={1:'train_acc',0:'test_acc'}\n",
    "    predall,yall=torch.tensor([]),torch.tensor([])\n",
    "    model.eval()\n",
    "    for x1,x2,y in tset:\n",
    "        x1,x2,y=Variable(x1.long()).to(device),Variable(x2.long()).to(device),Variable(y.long()).to(device)\n",
    "        pred=model(x1,x2,fea,G1).data\n",
    "        predall=torch.cat([predall,torch.as_tensor(pred,device='cpu')],dim=0)\n",
    "        yall=torch.cat([yall,torch.as_tensor(y,device='cpu')])\n",
    "        a=torch.max(pred,1)[1]\n",
    "        total+=y.size(0)\n",
    "        correct+=(a==y).sum()\n",
    "    print(st[string]+str((correct/total).item()))\n",
    "    torch.save((predall,yall),'PandY%d' % cros)\n",
    "'''\n",
    "def calculate_TPR_FPR(RD, f, B):\n",
    "    old_id = np.argsort(-RD)\n",
    "    min_f = int(min(f))\n",
    "    max_f = int(max(f))\n",
    "    TP_FN = np.zeros((RD.shape[0], 1), dtype=np.float64)\n",
    "    FP_TN = np.zeros((RD.shape[0], 1), dtype=np.float64)\n",
    "    TP = np.zeros((RD.shape[0], max_f), dtype=np.float64)\n",
    "    TP2 = np.zeros((RD.shape[0], min_f), dtype=np.float64)\n",
    "    FP = np.zeros((RD.shape[0], max_f), dtype=np.float64)\n",
    "    FP2 = np.zeros((RD.shape[0], min_f), dtype=np.float64)\n",
    "    P = np.zeros((RD.shape[0], max_f), dtype=np.float64)\n",
    "    P2 = np.zeros((RD.shape[0], min_f), dtype=np.float64)\n",
    "    for i in range(RD.shape[0]):\n",
    "        TP_FN[i] = sum(B[i] == 1)\n",
    "        FP_TN[i] = sum(B[i] == 0)\n",
    "    for i in range(RD.shape[0]):\n",
    "        for j in range(int(f[i])):\n",
    "            if j == 0:\n",
    "                if B[i][old_id[i][j]] == 1:\n",
    "                    FP[i][j] = 0\n",
    "                    TP[i][j] = 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "                else:\n",
    "                    TP[i][j] = 0\n",
    "                    FP[i][j] = 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "            else:\n",
    "                if B[i][old_id[i][j]] == 1:\n",
    "                    FP[i][j] = FP[i][j - 1]\n",
    "                    TP[i][j] = TP[i][j - 1] + 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "                else:\n",
    "                    TP[i][j] = TP[i][j - 1]\n",
    "                    FP[i][j] = FP[i][j - 1] + 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "    ki = 0\n",
    "    for i in range(RD.shape[0]):\n",
    "        if TP_FN[i] == 0:\n",
    "            TP[i] = 0\n",
    "            FP[i] = 0\n",
    "            ki = ki + 1\n",
    "        else:\n",
    "            TP[i] = TP[i] / TP_FN[i]\n",
    "            FP[i] = FP[i] / FP_TN[i]\n",
    "    for i in range(RD.shape[0]):\n",
    "        kk = f[i] / min_f\n",
    "        for j in range(min_f):\n",
    "            TP2[i][j] = TP[i][int(np.round_(((j + 1) * kk))) - 1]\n",
    "            FP2[i][j] = FP[i][int(np.round_(((j + 1) * kk))) - 1]\n",
    "            P2[i][j] = P[i][int(np.round_(((j + 1) * kk))) - 1]\n",
    "    TPR = TP2.sum(0) / (TP.shape[0] - ki)\n",
    "    FPR = FP2.sum(0) / (FP.shape[0] - ki)\n",
    "    Pr = P2.sum(0) / (P.shape[0] - ki)\n",
    "    return TPR, FPR, Pr\n",
    "def tacc(model,tset,fea,G1,tei,cros,Amax):\n",
    "    predall,yall=torch.tensor([]),torch.tensor([])\n",
    "    model.eval()\n",
    "    for x1,x2,y in tset:\n",
    "        x1,x2,y=Variable(x1.long()).to(device),Variable(x2.long()).to(device),Variable(y.long()).to(device)\n",
    "        pred=model(x1,x2,fea,G1).data\n",
    "        predall=torch.cat([predall,torch.as_tensor(pred,device='cpu')],dim=0)\n",
    "        yall=torch.cat([yall,torch.as_tensor(y,device='cpu')])\n",
    "    #torch.save((predall,yall),'./prady/PandY%d_%d' % (cros,epoch+1))\n",
    "    pred=torch.softmax(predall,dim=1)[:,1]\n",
    "    trh=torch.zeros(ld.shape[0],ld.shape[1])-1\n",
    "    tlh=torch.zeros(ld.shape[0],ld.shape[1])-1\n",
    "    trh[tei[0],tei[1]]=pred\n",
    "    tlh[tei[0],tei[1]]=yall\n",
    "    R=trh.numpy()\n",
    "    label=tlh.numpy()\n",
    "    f = np.zeros(shape=(R.shape[0], 1))\n",
    "    for i in range(R.shape[0]):\n",
    "        f[i] = np.sum(R[i] > -1)\n",
    "    if min(f)>0:\n",
    "        TPR,FPR,P=calculate_TPR_FPR(R,f,label)\n",
    "        AUC=metrics.auc(FPR, TPR)\n",
    "        AUPR=metrics.auc(TPR, P) + (TPR[0] * P[0])\n",
    "        print(\"AUC:%.4f_AUPR:%.4f\"%(AUC,AUPR))\n",
    "        if AUPR>Amax[1]:\n",
    "            Amax[0]=AUC\n",
    "            Amax[1]=AUPR\n",
    "            print(\"save\")\n",
    "            torch.save((predall,yall),\"PandY%d\"%cros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f916b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T01:28:59.326722Z",
     "start_time": "2022-10-07T00:56:51.652511Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    net=Net().to(device)\n",
    "    trset=DataLoader(MyDataset(tri[i],ld),batch,shuffle=True)\n",
    "    teset=DataLoader(MyDataset(tei[i],ld),batch,shuffle=False)\n",
    "    train_set,test_set=[],[]\n",
    "    for x1,x2,y in trset:\n",
    "        train_set.append((x1,x2,y))\n",
    "    for x1,x2,y in teset:\n",
    "        test_set.append((x1,x2,y))\n",
    "    print('cross:'+str(i+1))\n",
    "    train(net,train_set,test_set,feas[i],G1s[i],tei[i],epoch,learn_rate,i)\n",
    "    #torch.save(net.state_dict(),'./modelpara/model%d'%i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "718368e4",
   "metadata": {},
   "source": [
    "## plt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab41052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T13:28:23.342071Z",
     "start_time": "2022-10-06T13:28:23.312403Z"
    }
   },
   "outputs": [],
   "source": [
    "def fold_5(TPR, FPR, PR):\n",
    "    fold = len(TPR)\n",
    "    le = []\n",
    "    for i in range(fold):\n",
    "        le.append(len(TPR[i]))\n",
    "    min_f = min(le)\n",
    "    F_TPR = np.zeros((fold, min_f))\n",
    "    F_FPR = np.zeros((fold, min_f))\n",
    "    F_P = np.zeros((fold, min_f))\n",
    "    for i in range(fold):\n",
    "        k = len(TPR[i]) / min_f\n",
    "        for j in range(min_f):\n",
    "            F_TPR[i][j] = TPR[i][int(round(((j + 1) * k))) - 1]\n",
    "            F_FPR[i][j] = FPR[i][int(round(((j + 1) * k))) - 1]\n",
    "            F_P[i][j] = PR[i][int(round(((j + 1) * k))) - 1]\n",
    "    TPR_5 = F_TPR.sum(0) / fold\n",
    "    FPR_5 = F_FPR.sum(0) / fold\n",
    "    PR_5 = F_P.sum(0) / fold\n",
    "    return TPR_5, FPR_5, PR_5\n",
    "def calculate_TPR_FPR(RD, f, B):\n",
    "    old_id = np.argsort(-RD)\n",
    "    min_f = int(min(f))\n",
    "    max_f = int(max(f))\n",
    "    TP_FN = np.zeros((RD.shape[0], 1), dtype=np.float64)\n",
    "    FP_TN = np.zeros((RD.shape[0], 1), dtype=np.float64)\n",
    "    TP = np.zeros((RD.shape[0], max_f), dtype=np.float64)\n",
    "    TP2 = np.zeros((RD.shape[0], min_f), dtype=np.float64)\n",
    "    FP = np.zeros((RD.shape[0], max_f), dtype=np.float64)\n",
    "    FP2 = np.zeros((RD.shape[0], min_f), dtype=np.float64)\n",
    "    P = np.zeros((RD.shape[0], max_f), dtype=np.float64)\n",
    "    P2 = np.zeros((RD.shape[0], min_f), dtype=np.float64)\n",
    "    for i in range(RD.shape[0]):\n",
    "        TP_FN[i] = sum(B[i] == 1)\n",
    "        FP_TN[i] = sum(B[i] == 0)\n",
    "    for i in range(RD.shape[0]):\n",
    "        for j in range(int(f[i])):\n",
    "            if j == 0:\n",
    "                if B[i][old_id[i][j]] == 1:\n",
    "                    FP[i][j] = 0\n",
    "                    TP[i][j] = 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "                else:\n",
    "                    TP[i][j] = 0\n",
    "                    FP[i][j] = 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "            else:\n",
    "                if B[i][old_id[i][j]] == 1:\n",
    "                    FP[i][j] = FP[i][j - 1]\n",
    "                    TP[i][j] = TP[i][j - 1] + 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "                else:\n",
    "                    TP[i][j] = TP[i][j - 1]\n",
    "                    FP[i][j] = FP[i][j - 1] + 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "    ki = 0\n",
    "    for i in range(RD.shape[0]):\n",
    "        if TP_FN[i] == 0:\n",
    "            TP[i] = 0\n",
    "            FP[i] = 0\n",
    "            ki = ki + 1\n",
    "        else:\n",
    "            TP[i] = TP[i] / TP_FN[i]\n",
    "            FP[i] = FP[i] / FP_TN[i]\n",
    "    for i in range(RD.shape[0]):\n",
    "        kk = f[i] / min_f\n",
    "        for j in range(min_f):\n",
    "            TP2[i][j] = TP[i][int(np.round_(((j + 1) * kk))) - 1]\n",
    "            FP2[i][j] = FP[i][int(np.round_(((j + 1) * kk))) - 1]\n",
    "            P2[i][j] = P[i][int(np.round_(((j + 1) * kk))) - 1]\n",
    "    TPR = TP2.sum(0) / (TP.shape[0] - ki)\n",
    "    FPR = FP2.sum(0) / (FP.shape[0] - ki)\n",
    "    Pr = P2.sum(0) / (P.shape[0] - ki)\n",
    "    return TPR, FPR, Pr\n",
    "def curve(FPR, TPR, P):\n",
    "    plt.figure()\n",
    "    plt.subplot(121)\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.title(\"ROC curve  (AUC = %.4f)\" % (metrics.auc(FPR, TPR)))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.plot(FPR, TPR)\n",
    "    plt.subplot(122)\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.title(\"PR curve  (AUPR = %.4f)\" % (metrics.auc(TPR, P) + (TPR[0] * P[0])))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.plot(TPR, P)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6b82a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T13:28:23.515397Z",
     "start_time": "2022-10-06T13:28:23.505546Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_AUC_AUPR(Rh,labelh):\n",
    "    FPRs = []\n",
    "    TPRs = []\n",
    "    Ps = []\n",
    "    for step in range(5):\n",
    "        R=Rh[step]\n",
    "        label=labelh[step]\n",
    "        R=R\n",
    "        label=label\n",
    "        f = np.zeros(shape=(R.shape[0], 1))\n",
    "        for i in range(R.shape[0]):\n",
    "            f[i] = np.sum(R[i] > -1)\n",
    "        TPR, FPR, P = calculate_TPR_FPR(R, f, label)\n",
    "        FPRs.append(FPR)\n",
    "        TPRs.append(TPR)\n",
    "        Ps.append(P)\n",
    "    TPR_5, FPR_5, PR_5 = fold_5(TPRs, FPRs, Ps)\n",
    "    #np.savetxt(\"./data/compare/hyper/TPR.txt\",TPR_5)\n",
    "    #np.savetxt(\"./data/compare/hyper/FPR.txt\",FPR_5)\n",
    "    #np.savetxt(\"./data/compare/hyper/P.txt\",PR_5)\n",
    "    curve(FPR_5, TPR_5, PR_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8aafd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T13:28:33.212827Z",
     "start_time": "2022-10-06T13:28:23.702425Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n,ld,_,_,_,_,tei,_,_,_=torch.load('./parasave/tempdata2.pth')\n",
    "Rh,labelh=[],[]\n",
    "for i in range(n):\n",
    "    pred,y=torch.load('./PandY%d'%i)\n",
    "    pred=torch.softmax(pred,dim=1)[:,1]\n",
    "    trh=torch.zeros(ld.shape[0],ld.shape[1])-1\n",
    "    tlh=torch.zeros(ld.shape[0],ld.shape[1])-1\n",
    "    trh[tei[i][0],tei[i][1]]=pred\n",
    "    tlh[tei[i][0],tei[i][1]]=y\n",
    "    Rh.append(trh.numpy())\n",
    "    labelh.append(tlh.numpy())\n",
    "calculate_AUC_AUPR(Rh,labelh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58da70e0",
   "metadata": {},
   "source": [
    "# plt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from  scipy.stats import ttest_rel\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a7c56bc",
   "metadata": {},
   "source": [
    "## ROC and AUPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f440ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_TPR,hyper_FPR=np.loadtxt(\"./data/compare/hyper/TPR.txt\"),np.loadtxt(\"./data/compare/hyper/FPR.txt\")\n",
    "hyper_P,hyper_R=np.loadtxt(\"./data/compare/hyper/P.txt\"),np.loadtxt(\"./data/compare/hyper/TPR.txt\")\n",
    "GAIRD_TPR,GAIRD_FPR=np.loadtxt(\"./data/compare/GAIRD/TPR.txt\"),np.loadtxt(\"./data/compare/GAIRD/FPR.txt\")\n",
    "GAIRD_P,GAIRD_R=np.loadtxt(\"./data/compare/GAIRD/P.txt\"),np.loadtxt(\"./data/compare/GAIRD/TPR.txt\")\n",
    "GSMV_TPR,GSMV_FPR=np.loadtxt(\"./data/compare/GSMV/TPR.txt\"),np.loadtxt(\"./data/compare/GSMV/FPR.txt\")\n",
    "GSMV_P,GSMV_R=np.loadtxt(\"./data/compare/GSMV/P.txt\"),np.loadtxt(\"./data/compare/GSMV/TPR.txt\")\n",
    "MGLDA_TPR,MGLDA_FPR=np.loadtxt(\"./data/compare/MGLDA_Result/TPR.txt\"),np.loadtxt(\"./data/compare/MGLDA_Result/FPR.txt\")\n",
    "MGLDA_P,MGLDA_R=np.loadtxt(\"./data/compare/MGLDA_Result/P.txt\"),np.loadtxt(\"./data/compare/MGLDA_Result/TPR.txt\")\n",
    "GTAN_TPR,GTAN_FPR=np.loadtxt(\"./data/compare/GTAN_Result/TPR.txt\"),np.loadtxt(\"./data/compare/GTAN_Result/FPR.txt\")\n",
    "GTAN_P,GTAN_R=np.loadtxt(\"./data/compare/GTAN_Result/P.txt\"),np.loadtxt(\"./data/compare/GTAN_Result/R.txt\")\n",
    "VADLP_TPR,VADLP_FPR=np.loadtxt(\"./data/compare/VADLP_Result/VADLP_TPR.txt\"),np.loadtxt(\"./data/compare/VADLP_Result/VADLP_FPR.txt\")\n",
    "VADLP_P,VADLP_R=np.loadtxt(\"./data/compare/VADLP_Result/VADLP_P.txt\"),np.loadtxt(\"./data/compare/VADLP_Result/VADLP_TPR.txt\")\n",
    "CNNLDA_TPR,CNNLDA_FPR=np.loadtxt(\"./data/compare/Four_methods_Result/CNNLDA_TPR.txt\"),np.loadtxt(\"./data/compare/Four_methods_Result/CNNLDA_FPR.txt\")\n",
    "CNNLDA_P,CNNLDA_R=np.loadtxt(\"./data/compare/Four_methods_Result/CNNLDA_P.txt\"),np.loadtxt(\"./data/compare/Four_methods_Result/CNNLDA_TPR.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50133f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,5))\n",
    "ax=plt.subplot(121)\n",
    "a1, = plt.plot(hyper_FPR,hyper_TPR,'darkviolet')\n",
    "a2, = plt.plot(GAIRD_FPR,GAIRD_TPR,'y')\n",
    "a3, = plt.plot(GSMV_FPR,GSMV_TPR,'c')\n",
    "a4, = plt.plot(MGLDA_FPR,MGLDA_TPR,'pink')\n",
    "a5, = plt.plot(GTAN_FPR,GTAN_TPR,'g')\n",
    "a6, = plt.plot(VADLP_FPR,VADLP_TPR,'b')\n",
    "a7, = plt.plot(CNNLDA_FPR,CNNLDA_TPR,'r')\n",
    "plt.legend([a1,a2,a3,a4,a5,a6,a7],[\"AGLDA(%.3f)\"%(auc(hyper_FPR,hyper_TPR)),\n",
    "                                \"GAIRD(%.3f)\"%(auc(GAIRD_FPR,GAIRD_TPR)),\n",
    "                                \"GSMV(%.3f)\"%(auc(GSMV_FPR,GSMV_TPR)),\n",
    "                                \"MGLDA(%.3f)\"%(auc(MGLDA_FPR,MGLDA_TPR)),\n",
    "                                \"GTAN(%.3f)\"%(auc(GTAN_FPR,GTAN_TPR)),\n",
    "                                \"VADLP(%.3f)\"%(auc(VADLP_FPR,VADLP_TPR)),\n",
    "                                \"CNNLDA(%.3f)\"%(auc(CNNLDA_FPR,CNNLDA_TPR))],loc=\"best\",fontsize=6)\n",
    "ax.set_title(\"(A)ROC curves\",fontsize=10,)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "#fig.savefig(\"./data/compare/hyper/AUC.svg\",format=\"svg\")\n",
    "ax1=plt.subplot(122)\n",
    "a1, = plt.plot(hyper_R,hyper_P,'darkviolet')\n",
    "a2, = plt.plot(GAIRD_R,GAIRD_P,'y')\n",
    "a3, = plt.plot(GSMV_R,GSMV_P,'c')\n",
    "a4, = plt.plot(MGLDA_R,MGLDA_P,'pink')\n",
    "a5, = plt.plot(GTAN_R,GTAN_P,'g')\n",
    "a6, = plt.plot(VADLP_R,VADLP_P,'b')\n",
    "a7, = plt.plot(CNNLDA_R,CNNLDA_P,'r')\n",
    "plt.legend([a1,a2,a3,a4,a5,a6,a7],[\"AGLDA(%.3f)\"%(auc(hyper_R,hyper_P)+hyper_R[0]*hyper_P[0]),\n",
    "                                   \"GAIRD(%.3f)\"%(auc(GAIRD_R,GAIRD_P)+GAIRD_R[0]*GAIRD_P[0]),\n",
    "                                \"GSMV(%.3f)\"%(auc(GSMV_R,GSMV_P)+GSMV_R[0]*GSMV_P[0]),\n",
    "                                \"MGLDA(%.3f)\"%(auc(MGLDA_R,MGLDA_P)+MGLDA_R[0]*MGLDA_P[0]),\n",
    "                                \"GTAN(%.3f)\"%(auc(GTAN_R,GTAN_P)+GTAN_R[0]*GTAN_P[0]),\n",
    "                                \"VADLP(%.3f)\"%(auc(VADLP_R,VADLP_P)+VADLP_R[0]*VADLP_P[0]),\n",
    "                                \"CNNLDA(%.3f)\"%(auc(CNNLDA_R,CNNLDA_P)+CNNLDA_R[0]*CNNLDA_P[0])],loc=\"best\",fontsize=6)\n",
    "ax1.set_title(\"(B)PR curves\",fontsize=10,)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.plot(linewidth=15)\n",
    "plt.show()\n",
    "fig.savefig(\"./data/compare/hyper/AUCandAUPR.svg\",format=\"svg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c1d4a80",
   "metadata": {},
   "source": [
    "## top k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_Recall=[0.94173030,0.96624893,0.97417315,0.98944983,0.99286410,0.99367440,0.99626938,1]\n",
    "GAIRD_Rcall=[0.94024780,0.96510977,0.97338228,0.98642203,0.99186745,0.99565807,0.99815678,1]\n",
    "GSMV_Recall=[0.92372811,0.94158490,0.96736856,0.98186581,0.99045652,0.99177033,0.99302148,1]\n",
    "MGLDA_Recall=[0.91193719,0.93337950,0.96638335,0.97307928,0.98364267,0.99065807,0.99167848,1]\n",
    "GTAN_Recall  =  [0.89719380, 0.92813318, 0.94834836, 0.96815835, 0.97670324, 0.98511180, 0.98977049, 1.]\n",
    "VADLP_Recall = [0.85157542, 0.91083547, 0.92468019, 0.94498560, 0.97638439, 0.97972933, 0.98801061, 1.]\n",
    "CNNLDA_Recall = [0.74630222, 0.8952459, 0.91343092, 0.93538218, 0.97359227, 0.9738827, 0.97458565, 1.]\n",
    "fig = plt.figure(figsize=(7,5),dpi=100)#\n",
    "ax2 = fig.add_subplot(1,1,1)\n",
    "size = 8\n",
    "x = np.arange(size)\n",
    "total_width, n = 0.8, 8 \n",
    "width = total_width / n\n",
    "x = x - (total_width - width) / 2\n",
    "ax2.set_xticks(x)\n",
    "plt.bar(x - 0.3, hyper_Recall, width=width, label='AGLDA', color='darkviolet')\n",
    "plt.bar(x - 0.3 + 1*width, GAIRD_Rcall, width=width, label='GAIRD', color='g') \n",
    "plt.bar(x - 0.3 + 2*width, GSMV_Recall, width=width, label='GSMV', color='b') \n",
    "plt.bar(x - 0.3 + 3 *width, MGLDA_Recall, width=width, label='MGLDA', color='r') \n",
    "plt.bar(x - 0.3 + 4 * width, GTAN_Recall, width=width, label='GTAN', color='y') \n",
    "plt.bar(x - 0.3 + 5 * width, VADLP_Recall, width=width, label=\"VADLP\", color='c')\n",
    "plt.bar(x - 0.3 + 6 * width, CNNLDA_Recall, width=width, label='CNNLDA', color='pink')\n",
    "ax2.set_xticklabels(['Top30', 'Top60', 'Top90', 'Top120', 'Top150', 'Top180', 'Top210', 'Top240', ])\n",
    "box = ax2.get_position()\n",
    "ax2.set_position([box.x0, box.y0, box.width, box.height * 1.1])\n",
    "ax2.legend(loc='upper left', bbox_to_anchor=(0.06, 1.14),ncol=4)\n",
    "#plt.legend(loc='NorthOutside')\n",
    "plt.ylabel(\"Recall\",fontsize='large')\n",
    "plt.show()\n",
    "fig.savefig(\"./data/compare/hyper/top_k.svg\",format=\"svg\",bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbea0a0a",
   "metadata": {},
   "source": [
    "## p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b985bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,ld,dd,md,lm,tri,tei,lda,ll,mm=torch.load('./parasave/tempdata2.pth')\n",
    "numm=torch.zeros(240,405)\n",
    "predld=torch.zeros(240,405)\n",
    "yld=torch.zeros(240,405)\n",
    "for i in range(n):\n",
    "    pred,y=torch.load('./parasave/PandY%d_9877_6841'%i)\n",
    "    pred=torch.softmax(pred,dim=1)[:,1]\n",
    "    trh=torch.zeros(ld.shape[0],ld.shape[1])\n",
    "    tlh=torch.zeros(ld.shape[0],ld.shape[1])\n",
    "    numt=torch.zeros(ld.shape[0],ld.shape[1])\n",
    "    trh[tei[i][0],tei[i][1]]=pred\n",
    "    tlh[tei[i][0],tei[i][1]]=y\n",
    "    numt[tei[i][0],tei[i][1]]=1\n",
    "    numm+=numt\n",
    "    predld+=trh\n",
    "    yld+=tlh\n",
    "predld[35,225],predld[216,346]=0,0.9522#去除nan\n",
    "predld=(predld/numm).numpy()\n",
    "yld=(yld/numm).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_10_TPR_FPR_PR(F, index, dis_mi):\n",
    "    SS = np.argsort(-F)\n",
    "    v, w = [], []\n",
    "    for i in range(len(index)):\n",
    "        Precision, TPR, FPR = [], [], []\n",
    "        precisim_num, TPR_NUM, FPR_NUM = 0.0, 0.0, 0.0\n",
    "        b = sum(F[index[i]] > 0)\n",
    "        # print(b)\n",
    "        c = sum(dis_mi[index[i]] == 1)\n",
    "        # print(c)\n",
    "        d = b - c\n",
    "        J = 0\n",
    "        for j in range(b):\n",
    "            if dis_mi[index[i], SS[index[i], j]] == 1:\n",
    "                J += 1\n",
    "                TPR_NUM += 1 / c\n",
    "                TPR.append(TPR_NUM)\n",
    "                FPR.append(FPR_NUM)\n",
    "                Precision.append(J / (j + 1))\n",
    "            else:\n",
    "                FPR_NUM += 1 / d\n",
    "                TPR.append(TPR_NUM)\n",
    "                FPR.append(FPR_NUM)\n",
    "                Precision.append(J / (j + 1))\n",
    "        v.append(auc(FPR, TPR))\n",
    "        w.append(auc(TPR, Precision))\n",
    "    return v,w\n",
    "dis_10=[9,61,62,69,113,140,178,187,211,233]\n",
    "#n,ld,_,_,_,_,tei,_,_,_=torch.load('./parasave/tempdata2.pth')\n",
    "#dis_10_ROCs,dis_10_PRs=[0]*10,[0]*10\n",
    "#for i in range(n):\n",
    "#    pred,y=torch.load('./parasave/PandY%d_9877_6841'%i)\n",
    "#    pred=torch.softmax(pred,dim=1)[:,1]\n",
    "#    trh=torch.zeros(ld.shape[0],ld.shape[1])\n",
    "#    tlh=torch.zeros(ld.shape[0],ld.shape[1])\n",
    "#    trh[tei[i][0],tei[i][1]]=pred\n",
    "#    tlh[tei[i][0],tei[i][1]]=y\n",
    "#    lnc_dis1=tlh.numpy()\n",
    "#    Score_lnc_dis=trh.numpy()\n",
    "#    dis_10_ROC,dis_10_PR=calculate_10_TPR_FPR_PR(lnc_dis1.T,dis_10,Score_lnc_dis.T)\n",
    "#    dis_10_ROCs=[dis_10_ROCs[i]+dis_10_ROC[i] for i in range(10)]\n",
    "#    dis_10_PRs=[dis_10_PRs[i]+dis_10_PR[i] for i in range(10)]\n",
    "#hyper_auc=[dis_10_ROCs[i]/n for i in range(10)]\n",
    "#hyper_aupr=[dis_10_PRs[i]/n for i in range(10)]\n",
    "dis_10_ROC,dis_10_PR=calculate_10_TPR_FPR_PR(yld.T,dis_10,predld.T)\n",
    "#print(dis_10_ROC)\n",
    "#print(dis_10_PR)\n",
    "hyper_auc=dis_10_ROC\n",
    "hyper_aupr=dis_10_PR\n",
    "GAIRD_auc=[1,0.922,1,0.981,0.914,1,0.922,0.975,1,1]\n",
    "GAIRD_aupr=[0.438,0.091,0.531,1,0.425,1,0.781,0.626,0.361,0.894]\n",
    "GSMV_auc=[1,1,0.829,0.830,0.999,0.900,1,0.940,1,0.870]\n",
    "GSMV_aupr=[1,1,0.860,0.860,0.974,0.974,0.954,0.754,0.766,0.980]\n",
    "MGLDA_auc=[1,1,0.997,0.986,0.999,0.998,0.999,1,1,0.997]\n",
    "MGLDA_aupr=[1,1,0.850,0.840,0.974,0.964,0.944,0.744,0.736,0.970]\n",
    "GATN_auc=[0.994,0.987,0.996,1.000,0.984,0.990,0.992,1.000,0.974,0.995]\n",
    "GATN_aupr=[0.924,0.553,0.727,0.500,0.642,0.652,0.593,0.667,0.758,0.793]\n",
    "VADLP_auc=[0.951,0.934,0.963,0.984,0.975,0.978,0.985,0.981,0.961,0.893]\n",
    "VADLP_aupr=[0.372,0.203,0.624,0.238,0.461,0.825,0.837,0.628,0.37,0.205]\n",
    "CNNLDA_auc=[0.928,0.922,0.886,0.981,0.914,0.953,0.86,0.975,0.932,0.932]\n",
    "CNNLDA_aupr=[0.338,0.091,0.531,0.709,0.425,0.83,0.781,0.526,0.361,0.394]\n",
    "auc1 = ttest_rel(hyper_auc, GAIRD_auc)\n",
    "aupr1 = ttest_rel(hyper_aupr, GAIRD_aupr)\n",
    "auc2 = ttest_rel(hyper_auc, GSMV_auc)\n",
    "aupr2 = ttest_rel(hyper_aupr, GSMV_aupr)\n",
    "auc3 = ttest_rel(hyper_auc, MGLDA_auc)\n",
    "aupr3 = ttest_rel(hyper_aupr, MGLDA_aupr)\n",
    "auc4 = ttest_rel(hyper_auc, GATN_auc)\n",
    "aupr4 = ttest_rel(hyper_aupr, GATN_aupr)\n",
    "auc5 = ttest_rel(hyper_auc, VADLP_auc)\n",
    "aupr5 = ttest_rel(hyper_aupr, VADLP_aupr)\n",
    "auc6 = ttest_rel(hyper_auc, CNNLDA_auc)\n",
    "aupr6 = ttest_rel(hyper_aupr, CNNLDA_aupr)\n",
    "auc1,aupr1,auc2,aupr2,auc3,aupr3,auc4,aupr4,auc5,aupr5,auc6,aupr6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee03e4f1",
   "metadata": {},
   "source": [
    "## ablation test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "624dcbb6",
   "metadata": {},
   "source": [
    "### dismiss biological characteristic topological learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58883052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.c11=nn.Conv2d(1,32,kernel_size=(2,7),stride=1,padding=0)\n",
    "        self.s2=nn.MaxPool2d(kernel_size=(1,7))\n",
    "        self.c31=nn.Conv2d(32,64,kernel_size=(1,7),stride=1,padding=0)\n",
    "        self.c32=nn.Conv2d(32,64,kernel_size=(1,7),stride=1,padding=0)\n",
    "        self.s4=nn.MaxPool2d(kernel_size=(1,7))\n",
    "        self.cf1=nn.Linear(22*64,200)#1140->70\n",
    "        self.f3=nn.Linear(200,2)\n",
    "        self.cd1=nn.Dropout(0.5)\n",
    "        self.cd2=nn.Dropout2d(0.5)\n",
    "        self.leakrelu=nn.LeakyReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "    def forward(self,x1,x2,fea,G1):\n",
    "        x3=x2+240\n",
    "        x=torch.cat([fea[x1][:,None,None,:],fea[x3][:,None,None,:]],dim=2)\n",
    "        x=self.s2(self.leakrelu(self.c11(x)))\n",
    "        att=self.tanh(self.c32(x))\n",
    "        x=self.s4(self.leakrelu(self.c31(x)*att+self.c31(x)))\n",
    "        x=self.cd2(x)\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        x=self.leakrelu(self.cf1(x))\n",
    "        x=self.cd1(x)\n",
    "        x=self.f3(x)\n",
    "        return x\n",
    "#net=Net()\n",
    "#fea=torch.randn(1140,1140)\n",
    "#G=torch.randn(1140,1140)\n",
    "#x1=torch.linspace(0,31,32).long()\n",
    "#x2=x1\n",
    "#net(x1,x2,fea,G).shape\n",
    "##0.9869   0.6673"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a1fc321",
   "metadata": {},
   "source": [
    "### dismiss pairwise node attribute learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_conv(nn.Module):\n",
    "    '''\n",
    "    @article{feng2018hypergraph,\n",
    "    title={Hypergraph Neural Networks},\n",
    "    author={Feng, Yifan and You, Haoxuan and Zhang, Zizhao and Ji, Rongrong and Gao, Yue},\n",
    "    journal={AAAI 2019},\n",
    "    year={2018}\n",
    "    }\n",
    "    '''\n",
    "    def __init__(self, in_ft, out_ft, bias=True):\n",
    "        super(HGNN_conv, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_ft, out_ft))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_ft))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))  \n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, G: torch.Tensor):\n",
    "        x = x.matmul(self.weight)     \n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        x = G.matmul(x)          \n",
    "        return x\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.ne=nn.Parameter(torch.rand(500,300))\n",
    "        self.w1=nn.Parameter(torch.rand(1140,300))\n",
    "        #self.w2=nn.Parameter(torch.randn(1440,300))\n",
    "        self.w2=nn.Linear(1440,300)\n",
    "        self.h3=HGNN_conv(1140,600)\n",
    "        self.h4=HGNN_conv(600,300)\n",
    "        self.h1=HGNN_conv(1140,600)\n",
    "        self.h2=HGNN_conv(600,300)        \n",
    "        self.hf1=nn.Linear(1440*2,50)\n",
    "        self.f3=nn.Linear(50,2)\n",
    "        self.cd1=nn.Dropout(0.5)\n",
    "        self.cd2=nn.Dropout2d(0.5)\n",
    "        \n",
    "        self.leakrelu=nn.LeakyReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "    def forward(self,x1,x2,fea,G1):\n",
    "        \n",
    "        x3=x2+240\n",
    "\n",
    "        hyperH=self.leakrelu(G1@self.w1@self.ne.T)\n",
    "        self.H=hyperH\n",
    "        hyperH=(hyperH-hyperH.min())/(hyperH.max()-hyperH.min())\n",
    "        \n",
    "        dv=(torch.sum(hyperH,dim=1)**(-1/2))[:,None]\n",
    "        de=(torch.sum(hyperH,dim=0)**(-1))[None,:]\n",
    "        dv[torch.where(dv==torch.inf)]=0\n",
    "        de[torch.where(de==torch.inf)]=0\n",
    "        hyperH=dv*hyperH*de@hyperH.T*dv\n",
    "\n",
    "        fea1=self.leakrelu(self.h1(fea,G1))\n",
    "        fea2=self.leakrelu(self.h3(fea,hyperH))\n",
    "        fea3=self.leakrelu(self.h2(fea1,G1))\n",
    "        fea4=self.leakrelu(self.h4(fea2+fea1,hyperH))\n",
    "        fea3=torch.cat([fea,fea3+fea4],dim=1)\n",
    "\n",
    "        self.oe=self.w2(self.H.T@fea3)\n",
    "        \n",
    "        fea3=torch.cat([fea3[x1],fea3[x3]],dim=1)\n",
    "        fea3=self.leakrelu(self.hf1(fea3))\n",
    "        x=self.cd1(fea3)\n",
    "        x=self.f3(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self):\n",
    "        p=torch.softmax(self.oe/1e+7,dim=-1)\n",
    "        q=torch.log_softmax(self.ne,dim=-1)\n",
    "        return nn.KLDivLoss(reduction='batchmean')(q,p)\n",
    "#net=Net()\n",
    "#fea=torch.randn(1140,1140)\n",
    "#G=torch.randn(1140,1140)\n",
    "#x1=torch.linspace(0,31,32).long()\n",
    "#x2=x1\n",
    "#net(x1,x2,fea,G).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30cd6ca2",
   "metadata": {},
   "source": [
    "### dismiss self-learning hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_conv(nn.Module):\n",
    "    '''\n",
    "    @article{feng2018hypergraph,\n",
    "    title={Hypergraph Neural Networks},\n",
    "    author={Feng, Yifan and You, Haoxuan and Zhang, Zizhao and Ji, Rongrong and Gao, Yue},\n",
    "    journal={AAAI 2019},\n",
    "    year={2018}\n",
    "    }\n",
    "    '''\n",
    "    def __init__(self, in_ft, out_ft, bias=True):\n",
    "        super(HGNN_conv, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_ft, out_ft))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_ft))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))  \n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, G: torch.Tensor):\n",
    "        x = x.matmul(self.weight)     \n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        x = G.matmul(x)          \n",
    "        return x\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        self.h1=HGNN_conv(1140,600)\n",
    "        self.h2=HGNN_conv(600,300)        \n",
    "        self.hf1=nn.Linear(1440*2,50)\n",
    "\n",
    "        self.c11=nn.Conv2d(1,32,kernel_size=(2,7),stride=1,padding=0)\n",
    "        self.s2=nn.MaxPool2d(kernel_size=(1,7))\n",
    "        self.c31=nn.Conv2d(32,64,kernel_size=(1,7),stride=1,padding=0)\n",
    "        self.c32=nn.Conv2d(32,64,kernel_size=(1,7),stride=1,padding=0)\n",
    "        self.s4=nn.MaxPool2d(kernel_size=(1,7))\n",
    "        self.cf1=nn.Linear(22*64,200)#1140->70\n",
    "\n",
    "        self.f3=nn.Linear(250,2)\n",
    "        self.cd1=nn.Dropout(0.5)\n",
    "        self.cd2=nn.Dropout2d(0.5)\n",
    "        \n",
    "        self.leakrelu=nn.LeakyReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "    def forward(self,x1,x2,fea,G1):\n",
    "        \n",
    "        x3=x2+240\n",
    "        x=torch.cat([fea[x1][:,None,None,:],fea[x3][:,None,None,:]],dim=2)\n",
    "        \n",
    "        fea1=self.leakrelu(self.h1(fea,G1))\n",
    "        fea3=self.leakrelu(self.h2(fea1,G1))\n",
    "        fea3=torch.cat([fea,fea3],dim=1)\n",
    "        \n",
    "        fea3=torch.cat([fea3[x1],fea3[x3]],dim=1)\n",
    "        fea3=self.leakrelu(self.hf1(fea3))\n",
    "        \n",
    "        x=self.s2(self.leakrelu(self.c11(x)))\n",
    "        att=self.tanh(self.c32(x))\n",
    "        x=self.s4(self.leakrelu(self.c31(x)*att+self.c31(x)))\n",
    "        x=self.cd2(x)\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        x=self.leakrelu(self.cf1(x))\n",
    "        \n",
    "        x=torch.cat([x,fea3],dim=1)\n",
    "        \n",
    "        x=self.cd1(x)\n",
    "        x=self.f3(x)\n",
    "\n",
    "        return x\n",
    "#net=Net()\n",
    "#fea=torch.randn(1140,1140)\n",
    "#G=torch.randn(1140,1140)\n",
    "#x1=torch.linspace(0,31,32).long()\n",
    "#x2=x1\n",
    "#net(x1,x2,fea,G).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08e55b9b",
   "metadata": {},
   "source": [
    "### dismiss topological learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909115e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_conv(nn.Module):\n",
    "    '''\n",
    "    @article{feng2018hypergraph,\n",
    "    title={Hypergraph Neural Networks},\n",
    "    author={Feng, Yifan and You, Haoxuan and Zhang, Zizhao and Ji, Rongrong and Gao, Yue},\n",
    "    journal={AAAI 2019},\n",
    "    year={2018}\n",
    "    }\n",
    "    '''\n",
    "    def __init__(self, in_ft, out_ft, bias=True):\n",
    "        super(HGNN_conv, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_ft, out_ft))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_ft))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))  \n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, G: torch.Tensor):\n",
    "        x = x.matmul(self.weight)     \n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        x = G.matmul(x)          \n",
    "        return x\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.ne=nn.Parameter(torch.rand(500,300))\n",
    "        self.w1=nn.Parameter(torch.rand(1140,300))\n",
    "        self.w2=nn.Linear(1440,300)\n",
    "        self.h3=HGNN_conv(1140,600)\n",
    "        self.h4=HGNN_conv(600,300)\n",
    "        self.hf1=nn.Linear(1440*2,50)\n",
    "\n",
    "        self.c11=nn.Conv2d(1,32,kernel_size=(2,7),stride=1,padding=0)\n",
    "        self.s2=nn.MaxPool2d(kernel_size=(1,7))\n",
    "        self.c31=nn.Conv2d(32,64,kernel_size=(1,7),stride=1,padding=0)\n",
    "        self.c32=nn.Conv2d(32,64,kernel_size=(1,7),stride=1,padding=0)\n",
    "        self.s4=nn.MaxPool2d(kernel_size=(1,7))\n",
    "        self.cf1=nn.Linear(22*64,200)#1140->70\n",
    "\n",
    "        self.f3=nn.Linear(250,2)\n",
    "        self.cd1=nn.Dropout(0.5)\n",
    "        self.cd2=nn.Dropout2d(0.5)\n",
    "        \n",
    "        self.leakrelu=nn.LeakyReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "    def forward(self,x1,x2,fea,G1):\n",
    "        \n",
    "        x3=x2+240\n",
    "        x=torch.cat([fea[x1][:,None,None,:],fea[x3][:,None,None,:]],dim=2)\n",
    "        \n",
    "        hyperH=self.leakrelu(G1@self.w1@self.ne.T)\n",
    "        self.H=hyperH\n",
    "        hyperH=(hyperH-hyperH.min())/(hyperH.max()-hyperH.min())\n",
    "        \n",
    "        dv=(torch.sum(hyperH,dim=1)**(-1/2))[:,None]\n",
    "        de=(torch.sum(hyperH,dim=0)**(-1))[None,:]\n",
    "        dv[torch.where(dv==torch.inf)]=0\n",
    "        de[torch.where(de==torch.inf)]=0\n",
    "        hyperH=dv*hyperH*de@hyperH.T*dv\n",
    "\n",
    "        fea2=self.leakrelu(self.h3(fea,hyperH))\n",
    "        fea4=self.leakrelu(self.h4(fea2,hyperH))\n",
    "        fea3=torch.cat([fea,fea4],dim=1)\n",
    "\n",
    "        self.oe=self.w2(self.H.T@fea3)\n",
    "        \n",
    "        fea3=torch.cat([fea3[x1],fea3[x3]],dim=1)\n",
    "        fea3=self.leakrelu(self.hf1(fea3))\n",
    "        \n",
    "        x=self.s2(self.leakrelu(self.c11(x)))\n",
    "        att=self.tanh(self.c32(x))\n",
    "        x=self.s4(self.leakrelu(self.c31(x)*att+self.c31(x)))\n",
    "        x=self.cd2(x)\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        x=self.leakrelu(self.cf1(x))\n",
    "        \n",
    "        x=torch.cat([x,fea3],dim=1)\n",
    "        \n",
    "        x=self.cd1(x)\n",
    "        x=self.f3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def loss(self):\n",
    "        p=torch.softmax(self.oe/1e+7,dim=-1)\n",
    "        q=torch.log_softmax(self.ne,dim=-1)\n",
    "        return nn.KLDivLoss(reduction='batchmean')(q,p)\n",
    "#net=Net()\n",
    "#fea=torch.randn(1140,1140)\n",
    "#G=torch.randn(1140,1140)\n",
    "#x1=torch.linspace(0,31,32).long()\n",
    "#x2=x1\n",
    "#net(x1,x2,fea,G).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1897493b",
   "metadata": {},
   "source": [
    "### dismiss hyperedge update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb076a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_conv(nn.Module):\n",
    "    '''\n",
    "    @article{feng2018hypergraph,\n",
    "    title={Hypergraph Neural Networks},\n",
    "    author={Feng, Yifan and You, Haoxuan and Zhang, Zizhao and Ji, Rongrong and Gao, Yue},\n",
    "    journal={AAAI 2019},\n",
    "    year={2018}\n",
    "    }\n",
    "    '''\n",
    "    def __init__(self, in_ft, out_ft, bias=True):\n",
    "        super(HGNN_conv, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_ft, out_ft))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_ft))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))  \n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, G: torch.Tensor):\n",
    "        x = x.matmul(self.weight)     \n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        x = G.matmul(x)          \n",
    "        return x\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.ne=nn.Parameter(torch.rand(500,300))\n",
    "        self.w1=nn.Parameter(torch.rand(1140,300))\n",
    "        self.h3=HGNN_conv(1140,600)\n",
    "        self.h4=HGNN_conv(600,300)\n",
    "        self.h1=HGNN_conv(1140,600)\n",
    "        self.h2=HGNN_conv(600,300)        \n",
    "        self.hf1=nn.Linear(1440*2,50)\n",
    "\n",
    "        self.c11=nn.Conv2d(1,32,kernel_size=(2,7),stride=1,padding=0)\n",
    "        self.s2=nn.MaxPool2d(kernel_size=(1,7))\n",
    "        self.c31=nn.Conv2d(32,64,kernel_size=(1,7),stride=1,padding=0)\n",
    "        self.c32=nn.Conv2d(32,64,kernel_size=(1,7),stride=1,padding=0)\n",
    "        self.s4=nn.MaxPool2d(kernel_size=(1,7))\n",
    "        self.cf1=nn.Linear(22*64,200)#1140->70\n",
    "\n",
    "        self.f3=nn.Linear(250,2)\n",
    "        self.cd1=nn.Dropout(0.5)\n",
    "        self.cd2=nn.Dropout2d(0.5)\n",
    "        \n",
    "        self.leakrelu=nn.LeakyReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "    def forward(self,x1,x2,fea,G1):\n",
    "        \n",
    "        x3=x2+240\n",
    "        x=torch.cat([fea[x1][:,None,None,:],fea[x3][:,None,None,:]],dim=2)\n",
    "        \n",
    "        hyperH=self.leakrelu(G1@self.w1@self.ne.T)\n",
    "        self.H=hyperH\n",
    "        hyperH=(hyperH-hyperH.min())/(hyperH.max()-hyperH.min())\n",
    "        \n",
    "        dv=(torch.sum(hyperH,dim=1)**(-1/2))[:,None]\n",
    "        de=(torch.sum(hyperH,dim=0)**(-1))[None,:]\n",
    "        dv[torch.where(dv==torch.inf)]=0\n",
    "        de[torch.where(de==torch.inf)]=0\n",
    "        hyperH=dv*hyperH*de@hyperH.T*dv\n",
    "\n",
    "        fea1=self.leakrelu(self.h1(fea,G1))\n",
    "        fea2=self.leakrelu(self.h3(fea,hyperH))\n",
    "        fea3=self.leakrelu(self.h2(fea1,G1))\n",
    "        fea4=self.leakrelu(self.h4(fea2+fea1,hyperH))\n",
    "        fea3=torch.cat([fea,fea3+fea4],dim=1)\n",
    "  \n",
    "        fea3=torch.cat([fea3[x1],fea3[x3]],dim=1)\n",
    "        fea3=self.leakrelu(self.hf1(fea3))\n",
    "        \n",
    "        x=self.s2(self.leakrelu(self.c11(x)))\n",
    "        att=self.tanh(self.c32(x))\n",
    "        x=self.s4(self.leakrelu(self.c31(x)*att+self.c31(x)))\n",
    "        x=self.cd2(x)\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        x=self.leakrelu(self.cf1(x))\n",
    "        \n",
    "        x=torch.cat([x,fea3],dim=1)\n",
    "        \n",
    "        x=self.cd1(x)\n",
    "        x=self.f3(x)\n",
    "\n",
    "        return x\n",
    "#net=Net()\n",
    "#fea=torch.randn(1140,1140)\n",
    "#G=torch.randn(1140,1140)\n",
    "#x1=torch.linspace(0,31,32).long()\n",
    "#x2=x1\n",
    "#net(x1,x2,fea,G).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e544037",
   "metadata": {},
   "source": [
    "## 50 xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():  # lnc 240 dis 405 mi 495\n",
    "    ld=np.loadtxt(\"./data/lnc_dis_association.txt\",dtype=int)\n",
    "    dd=np.loadtxt(\"./data/dis_sim_matrix_process.txt\",dtype=float)\n",
    "    md=np.loadtxt(\"./data/mi_dis.txt\",dtype=int)\n",
    "    lm=np.loadtxt(\"./data/yuguoxian_lnc_mi.txt\",dtype=int)\n",
    "    ll=np.loadtxt(\"./data/lnc_sim.txt\",dtype=float)\n",
    "    return torch.tensor(ld),torch.tensor(dd),torch.tensor(md),torch.tensor(lm),torch.tensor(ll)\n",
    "def calculate_sim(ld,dd):\n",
    "    s1=ld.shape[0]\n",
    "    ll=torch.eye(s1)\n",
    "    m2=dd*ld[:,None,:]\n",
    "    m1=ld[:,:,None]\n",
    "    for x,y in itertools.permutations(torch.linspace(0,s1-1,s1,dtype=torch.long),2):\n",
    "        x,y=x.item(),y.item()\n",
    "        m=m1[x,:,:]*m2[y,:,:]\n",
    "        if ld[x].sum()+ld[y].sum()==0:\n",
    "            ll[x,y]=0\n",
    "        else:\n",
    "            ll[x,y]=(m.max(dim=0,keepdim=True)[0].sum()+m.max(dim=1,keepdim=True)[0].sum())/(ld[x].sum()+ld[y].sum())\n",
    "    return ll\n",
    "def cfm(ll,ld,dd,md,lm,mm):\n",
    "    r1=torch.cat([ll,ld,lm],dim=1)\n",
    "    r2=torch.cat([ld.T,dd,md.T],dim=1)\n",
    "    r3=torch.cat([lm.T,md,mm],dim=1)\n",
    "    fea=torch.cat([r1,r2,r3],dim=0)\n",
    "    deg=torch.diag((torch.sum(fea>0,dim=1))**(-1/2)).double()\n",
    "    G1=deg@fea@deg\n",
    "    return fea,G1\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,tri,ld):\n",
    "        self.tri=tri\n",
    "        self.ld=ld\n",
    "    def __getitem__(self,idx):\n",
    "        x,y=self.tri[idx,:]\n",
    "        label=self.ld[x][y]\n",
    "        return x,y,label\n",
    "    def __len__(self):\n",
    "        return self.tri.shape[0]\n",
    "batch=32\n",
    "ld,dd,md,lm,ll=load_data()\n",
    "mm=calculate_sim(md,dd)\n",
    "fea,G1=cfm(ll,ld,dd,md,lm,mm)\n",
    "ti=torch.argwhere(ld>-1)\n",
    "trset=DataLoader(MyDataset(ti,ld),batch,shuffle=True)\n",
    "teset=DataLoader(MyDataset(ti,ld),batch,shuffle=False)\n",
    "train_set,test_set=[],[]\n",
    "for x1,x2,y in trset:\n",
    "    train_set.append((x1,x2,y))\n",
    "for x1,x2,y in teset:\n",
    "    test_set.append((x1,x2,y))\n",
    "torch.save(train_set,'./data/compare/hyper/50xls/trainset.pth')\n",
    "torch.save(test_set,'./data/compare/hyper/50xls/testset.pth')\n",
    "torch.save([ti,G1,fea],'./data/compare/hyper/50xls/par.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_set,test_set,fea,G1,tei,epoch,learn_rate):\n",
    "    optimizer=torch.optim.Adam(model.parameters(),learn_rate,weight_decay=0.001)\n",
    "    cost=nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    fea,G1=fea.float().to(device),G1.float().to(device)\n",
    "    Amax=[0,0]\n",
    "    for i in range(epoch):\n",
    "        for x1,x2,y in train_set:\n",
    "            x1,x2,y=Variable(x1.long()).to(device),Variable(x2.long()).to(device),Variable(y.long()).to(device)\n",
    "            out=model(x1,x2,fea,G1)\n",
    "            loss=cost(out,y)+model.loss()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (i+1)%1 == 0: #and i+1>=50:\n",
    "            print(i)\n",
    "            tacc(model,test_set,fea,G1,tei,Amax)\n",
    "        #if i+1==epoch:\n",
    "            #tacc(model,test_set,fea,G1,0,cros)\n",
    "        torch.cuda.empty_cache()\n",
    "def calculate_TPR_FPR(RD, f, B):\n",
    "    old_id = np.argsort(-RD)\n",
    "    min_f = int(min(f))\n",
    "    max_f = int(max(f))\n",
    "    TP_FN = np.zeros((RD.shape[0], 1), dtype=np.float64)\n",
    "    FP_TN = np.zeros((RD.shape[0], 1), dtype=np.float64)\n",
    "    TP = np.zeros((RD.shape[0], max_f), dtype=np.float64)\n",
    "    TP2 = np.zeros((RD.shape[0], min_f), dtype=np.float64)\n",
    "    FP = np.zeros((RD.shape[0], max_f), dtype=np.float64)\n",
    "    FP2 = np.zeros((RD.shape[0], min_f), dtype=np.float64)\n",
    "    P = np.zeros((RD.shape[0], max_f), dtype=np.float64)\n",
    "    P2 = np.zeros((RD.shape[0], min_f), dtype=np.float64)\n",
    "    for i in range(RD.shape[0]):\n",
    "        TP_FN[i] = sum(B[i] == 1)\n",
    "        FP_TN[i] = sum(B[i] == 0)\n",
    "    for i in range(RD.shape[0]):\n",
    "        for j in range(int(f[i])):\n",
    "            if j == 0:\n",
    "                if B[i][old_id[i][j]] == 1:\n",
    "                    FP[i][j] = 0\n",
    "                    TP[i][j] = 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "                else:\n",
    "                    TP[i][j] = 0\n",
    "                    FP[i][j] = 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "            else:\n",
    "                if B[i][old_id[i][j]] == 1:\n",
    "                    FP[i][j] = FP[i][j - 1]\n",
    "                    TP[i][j] = TP[i][j - 1] + 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "                else:\n",
    "                    TP[i][j] = TP[i][j - 1]\n",
    "                    FP[i][j] = FP[i][j - 1] + 1\n",
    "                    P[i][j] = TP[i][j] / (j + 1)\n",
    "    ki = 0\n",
    "    for i in range(RD.shape[0]):\n",
    "        if TP_FN[i] == 0:\n",
    "            TP[i] = 0\n",
    "            FP[i] = 0\n",
    "            ki = ki + 1\n",
    "        else:\n",
    "            TP[i] = TP[i] / TP_FN[i]\n",
    "            FP[i] = FP[i] / FP_TN[i]\n",
    "    for i in range(RD.shape[0]):\n",
    "        kk = f[i] / min_f\n",
    "        for j in range(min_f):\n",
    "            TP2[i][j] = TP[i][int(np.round_(((j + 1) * kk))) - 1]\n",
    "            FP2[i][j] = FP[i][int(np.round_(((j + 1) * kk))) - 1]\n",
    "            P2[i][j] = P[i][int(np.round_(((j + 1) * kk))) - 1]\n",
    "    TPR = TP2.sum(0) / (TP.shape[0] - ki)\n",
    "    FPR = FP2.sum(0) / (FP.shape[0] - ki)\n",
    "    Pr = P2.sum(0) / (P.shape[0] - ki)\n",
    "    return TPR, FPR, Pr\n",
    "def tacc(model,tset,fea,G1,tei,Amax):\n",
    "    predall,yall=torch.tensor([]),torch.tensor([])\n",
    "    model.eval()\n",
    "    for x1,x2,y in tset:\n",
    "        x1,x2,y=Variable(x1.long()).to(device),Variable(x2.long()).to(device),Variable(y.long()).to(device)\n",
    "        pred=model(x1,x2,fea,G1).data\n",
    "        predall=torch.cat([predall,torch.as_tensor(pred,device='cpu')],dim=0)\n",
    "        yall=torch.cat([yall,torch.as_tensor(y,device='cpu')])\n",
    "    pred=torch.softmax(predall,dim=1)[:,1]\n",
    "    trh=torch.zeros(240,405)-1\n",
    "    tlh=torch.zeros(240,405)-1\n",
    "    trh[tei[:,0],tei[:,1]]=pred\n",
    "    tlh[tei[:,0],tei[:,1]]=yall\n",
    "    R=trh.numpy()\n",
    "    label=tlh.numpy()\n",
    "    f = np.zeros(shape=(R.shape[0], 1))\n",
    "    for i in range(R.shape[0]):\n",
    "        f[i] = np.sum(R[i] > -1)\n",
    "    if min(f)>0:\n",
    "        TPR,FPR,P=calculate_TPR_FPR(R,f,label)\n",
    "        AUC=metrics.auc(FPR, TPR)\n",
    "        AUPR=metrics.auc(TPR, P) + (TPR[0] * P[0])\n",
    "        print(\"AUC:%.4f_AUPR:%.4f\"%(AUC,AUPR))\n",
    "        if AUPR>Amax[1]:\n",
    "            Amax[0]=AUC\n",
    "            Amax[1]=AUPR\n",
    "            print(\"save\")\n",
    "            torch.save((predall,yall),\"PandY\")\n",
    "ti,G1,fea=torch.load('./data/compare/hyper/50xls/par.pth')\n",
    "train_set=torch.load('./data/compare/hyper/50xls/trainset.pth')\n",
    "test_set=torch.load('./data/compare/hyper/50xls/testset.pth')\n",
    "net=Net().to(device)\n",
    "train(net,train_set,test_set,fea,G1,ti,epoch=15,learn_rate=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32972b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n,ld,_,_,_,_,tei,_,_,_=torch.load('tempdata2.pth')\n",
    "ti,_,_=torch.load('./data/compare/hyper/50xls/par.pth')\n",
    "pred,_=torch.load('./PandY')\n",
    "pred=torch.softmax(pred,dim=1)[:,1]\n",
    "trh=torch.zeros(240,405)\n",
    "trh[ti[:,0],ti[:,1]]=pred\n",
    "lnc_dis_score=trh\n",
    "index = np.argsort(-lnc_dis_score, axis=0) \n",
    "lncRNA_name=np.loadtxt(\"./data/yuguoxian_lncRNA_name.txt\",dtype=str)\n",
    "dis_name=pd.read_csv(\"./data/disease_name.txt\",header=None,sep='\\t')\n",
    "lncRNA_all_50 = index[:30,:]\n",
    "lncRNA_name_50 = lncRNA_all_50.T\n",
    "candidate_list = []\n",
    "for i in range(lncRNA_name_50.shape[0]): # 405\n",
    "    for j in range(lncRNA_name_50.shape[1]): #50\n",
    "        candidate_list.append([dis_name[2][i],lncRNA_name[lncRNA_name_50[i][j]],j+1,lnc_dis_score[lncRNA_name_50[i][j]][i]])\n",
    "result = open('./data/compare/hyper/ST1.csv', 'w', encoding='gbk')\n",
    "result.write('Disease Name,Candidate lncRNA name,Rank,Association score\\n')\n",
    "for m in range(len(candidate_list)): # 遍历的是405 * 50 的长度\n",
    "    for n in range(len(candidate_list[m])): # 将每一行的元素求一下长度\n",
    "        result.write(str(candidate_list[m][n])) # 一个一个的写入\n",
    "        result.write(',') # 以 \\t结束一行的写入\n",
    "    result.write('\\n') # 换行重新写\n",
    "result.close() # 写完关闭文件"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82dbb57d",
   "metadata": {},
   "source": [
    "# Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169.825px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "236.85px",
    "left": "1211.6px",
    "right": "20px",
    "top": "111px",
    "width": "267px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
